1. 融合大模型首字耗时、并发情况

    服务并发情况：服务QPS峰值在200左右；其中大模型业务流量占比峰值每分钟800左右，也就是QPS:800/60 = 13qps左右，
    
    做服务压测时，一般是要求在30并发环境下压测，压测时实际服务qps跟请求平均耗时有关，不同业务处理请求的平均耗时差别比较大，例如DeepSeek的和感知问答，差别都比较大，DeepSeek的可能平均耗时在4.1秒左右，感知问答的可能560ms就结束了。

    车辆总在线数峰值在9.4万左右，
    
    与QPS差别较大的原因主要是由于客户端页面场景数据、设置项数据等，客户端需要主动上传业务场景数据或者设置项，所以可能出现频繁与云端建立连接的情况。


2. 闲聊百科业务指标
    闲聊百科：闲聊用的doubao-1.5-pro-32k

    首字耗时P98:1610ms，P90：1344ms，平均耗时984ms;官网调用首token耗时900多ms

3. 用车问答业务

    RAG业务流程如下：
    - query经过领域分类到用车问答业务，首先会通过知识手册向量化索引进行召回，获取相关的知识片段
    - 对召回结果基于开源的Bge rerank模型做排序，提高相关性
    - 同时如果召回匹配度不高，会做联网检索，获取额外信息
    - 最后会结合联网检索结果和知识手册召回结果，构造大模型prompt，给到大模型生成回复。


    用车问答：实际综合准确率90%左右，各车型准确率有细微差别，走联网占比：0.417%，P98:1085ms, P90:776ms,平均耗时627ms，向量数据库：AnalyticDB，做的预处理包括：query改写、车型改写等。检索方法：全文bge向量匹配及关键词bge向量匹配；排序方法使用的开源Bge rerank。后续优化方向：prompt话术精准调整（根据车型和检索类别等）；线下增加的回放、feedBack loop反馈闭环（额外设计收集对问答内容的反馈，基于反馈优化检索策略）能力

    RAG分块策略：
    - 固定大小分块：实现简单，但是重要信息可能被分到不同的块中
    - 语义分块：根据段落分段后，为每个分段做向量化相似度匹配，相似性较高的合并到一块
    - 带上下文信息的语义分块：是对语义分块的优化，分块结束后，会将前面和后面的快都放到当前块的metadata中，匹配时，将上下文的chunk和当前chunk都提供给大模型
    - 增加标题、问题等辅助信息：对每个分块添加描述性的标题，或者标准问、少量泛化问题，在查询时，不仅对原始文本做相似性计算，对标题和相关问题也做相似性计算，按一定权重返回最终得分。
    - 递归分块：基于段落、章节等固有分隔符，比如句号，进行分块，如果分块大小超过maxSize，则将该块拆分为更小的块，知道符合要求，否则一直递归拆分。（LangChain）
    - 基于大语言模型进行分块，大模型按照设定提示词，给出分块结果。
    
4. PE相关

    **填充模板**
    - jinja2：python版本的模板填充方案，也有对应的java版本，纯文本模板填充，不需要什么ai框架或者大模型依赖
    - spring-ai和langchain4j也都有提供promptTemplate填充相关的api调用


    **Prompt优化心得**
    - 明确指令与角色：给模型明确的身份和业务角色，并且说明要完成的任务
    - 提供结构化上下文：可以利用RAG检索知识库，检索结果和相应的辅助信息（例如位置、当前音乐等）结合结构化字段填充prompt中，而不是直接拼接原文
    - 控制长度与重要性：prompt长度要控制好，减少无关信息干扰，对重要信息做显示的标记例如感叹号或者明确优先级
    - 分步：prompt中可以将复杂问题目标拆成若干步，作为辅助
    - Few-shot示例：基于input和业务场景，提供几个示例的输入与预期回复，这样大模型更容易理解要求
    - 难缠Case：面向case优化，初版数据集 + 优化后，针对错误case定向分析，也可以用大模型，将SP与错误case丢给LLM，追问为什么不按prompt来，可能会有效果。

5. Agent框架相关

    **Agent框架了解**

    **React模式Agent**

    **OpenManus**

    整个项目基于Planning + ReAct模式的Agent设计，think阶段通过LLM分析需求和可用工具，Action阶段执行选定工具并观察结果，通过think-act循环逐步完成任务，再加上丰富的工具集扩展Agent能力。

    后续优化可以加上自我反思、错误重试的机制，定期触发反思或者出现错误后反思，例如增加一步：询问大模型基于之前的执行记录，有什么建议给出，用于更好的指导后续类似任务执行。反思总结的结果可以考虑存储起来

    *react模式单Agent*
    
    入口处，基于 max_steps 和 AgentState来做控制，max_steps 相当于是安全阀，最大执行步数；AgentState 规定了当前Agent的状态，例如IDLE（空闲）、RUNNING、FININSHED、ERROR。

    在Agent运行每个step的实现里，总得包括两个阶段：think() 和 act()

    think()阶段：将当前对话历史、可用工具打包，调用LLM；从LLM响应中解析出要调用的工具（tool_calls）和思考过程（content）；还需要将LLM的思考和规划存入memory中；最终根据LLM输出，决定是否需要进行下一步的行动（act）;think()阶段返回bool值，True代表（计划调用工具 或者有流式回复文本）Flase代表异常或者无响应，无需行动。

    act()阶段：

    首先会做一些无工具调用的情况判断，例如只有流式文本的输出；然后遍历tool_calls，执行工具调用；并且将工具执行结果记录到Memory中，最终汇总所有工具执行结果返回

    在Prompt中也明确告知了LLM，如果任务完成或者需要停止交互时，需要调用某工具，在工具调用实现中会更改AgentState，结束循环。防止不必要的计算和资源消耗

    存在的问题：将AgentStage状态改为FINISHED是通过调用完tool之后来触发的，如果错误地调用了tool，就直接结束了；可以结合多Agent协作来处理。


    *多Agent协作*
    首先通过FlowFactory创建完PlanningFlow工作流后，进行plan规划，在调用大模型做plan时，会将可用的agent name + description拼接到system prompt中，同时还会提供plan对应的tool，结构化的形式创建plan，提供Plan的创建、跟踪更新步骤、初始步骤等plan管理能力。

    拿到planningTool的结果后，循环执行每一个step，每个step type与agent相对应，根据step type拿到对应的agent后，进入到该agent的执行流程，任务执行过程中，任何一个Agent状态变为FINISHED，都表示任务执行完成并退出整个任务

